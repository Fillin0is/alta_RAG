import streamlit as st
from llm_connector import LLMConnector
from vector_store_pg import VectorStore
from config import EMBEDDING_PATH, DB_PARAMS
from pathlib import Path
import os


def initialize_system():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""

    if not Path(EMBEDDING_PATH).exists():
        st.error(f"–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {EMBEDDING_PATH}!")
        st.stop()

    with st.spinner("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã..."):
        db = VectorStore(EMBEDDING_PATH, DB_PARAMS)

    with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏..."):
        llm = LLMConnector()

    return llm, db

def chat_interface(llm, db):
    """–û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —á–∞—Ç–∞"""
    st.title("ü§ñ –õ–æ–∫–∞–ª—å–Ω—ã–π RAG-—á–∞—Ç")
    st.caption(f"–ú–æ–¥–µ–ª—å: Mistral 7B")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
    if prompt := st.chat_input("–í–∞—à –≤–æ–ø—Ä–æ—Å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            try:
                docs = db.hybrid_search(prompt)

                # st.subheader("üîé –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ FAISS:")
                # for i, d in enumerate(docs, 1):
                #     st.write(f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i}:")
                #     st.code(d['page_content'][:1000])

                context = "\n\n".join([
                    f'üìÑ [{d["metadata"]["type_document"].upper()} | {d["metadata"]["source"]}]\n{d["page_content"]}' 
                    for d in docs
                ])
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
                message_placeholder = st.empty()
                full_response = ""
                
                for chunk in llm.generate_response(prompt, context):
                    full_response += chunk
                    message_placeholder.markdown(full_response + "‚ñå")
                
                message_placeholder.markdown(full_response)
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
                raise  # –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            
def main():
    st.set_page_config(page_title="–õ–æ–∫–∞–ª—å–Ω—ã–π RAG-—á–∞—Ç", layout="wide")
    
    with st.container():
        st.write("## –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã...")
        llm, db = initialize_system()
        st.success("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        
    chat_interface(llm, db)

if __name__ == "__main__":
    main()