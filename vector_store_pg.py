import psycopg2
from pgvector.psycopg2 import register_vector
from pgvector.vector import Vector
from psycopg2.extras import Json
from sentence_transformers import SentenceTransformer
from llm_connector import EmbedConnector
from typing import List, Dict
from tqdm import tqdm
import hashlib


class VectorStore:
    def __init__(self, embed_path: str, db_params: Dict):
        """
        db_params = {
            "dbname": "...",
            "user": "...",
            "password": "...",
            "host": "...",
            "port": 5432
        }
        """
        self.model = EmbedConnector()
        self.conn = psycopg2.connect(**db_params)
        register_vector(self.conn)

        self._init_table()
        

    def _init_table(self):  
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –µ–µ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏
        """
        with self.conn.cursor() as cursor:
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS files (
                    id BIGSERIAL PRIMARY KEY,
                    source TEXT UNIQUE NOT NULL,       -- –ø—É—Ç—å –∏–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞
                    file_hash TEXT NOT NULL,           -- sha256 —Ñ–∞–π–ª–∞
                    processed_at TIMESTAMP NOT NULL DEFAULT NOW()
                );

                CREATE TABLE IF NOT EXISTS documents (
                    id BIGSERIAL PRIMARY KEY,
                    id_file BIGINT NOT NULL REFERENCES files(id) ON DELETE CASCADE, -- —Å–≤—è–∑—å —Å —Ñ–∞–π–ª–æ–º
                    content TEXT NOT NULL,
                    embedding vector(1536) NOT NULL,
                    metadata JSONB NOT NULL
                );

                -- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
                CREATE INDEX IF NOT EXISTS idx_documents_file ON documents(id_file);
                CREATE INDEX IF NOT EXISTS idx_documents_embedding ON documents USING ivfflat (embedding vector_cosine_ops);
                CREATE INDEX IF NOT EXISTS idx_documents_tsv ON documents USING gin (to_tsvector('simple', content));
                """
            )
        
        self.conn.commit()

    def _hash_content(self, text: str) -> str:
        """–í—ã—á–∏—Å–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ö—ç—à –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        return hashlib.sha256(text.encode("utf-8")).hexdigest()
        
    def create_index(self, texts: List[Dict], file_path: str):
        """
        texts = [{"page_content": "...", "metadata": {...}}, ...]
        file_path = –ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É 

        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1. –°—á–∏—Ç–∞–µ–º —Ö—ç—à —Ñ–∞–π–ª–∞
        2. –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ –µ—Å—Ç—å –≤ –ë–î –∏ —Ö—ç—à —Å–æ–≤–ø–∞–¥–∞–µ—Ç -> –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        3. –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç –∏–ª–∏ –¥—Ä—É–≥–æ–π —Ö—ç—à:
            - —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏;
            - –≤—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ.
        """
        if not texts:
            raise ValueError("–ù–µ –ø–µ—Ä–µ–¥–∞–Ω—ã —Ç–µ–∫—Å—Ç—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        
        # 1. –°—á–∏—Ç–∞–µ–º —Ö—ç—à —Ñ–∞–π–ª–∞
        with open(file_path, "rb") as f:
            file_bytes = f.read()
        file_hash = hashlib.sha256(file_bytes).hexdigest()

        with self.conn.cursor() as cursor:
            # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π —Ñ–∞–π–ª
            cursor.execute(
                "SELECT id, file_hash FROM files WHERE source = %s;",
                (file_path,)
            )
            row = cursor.fetchone()

            if row and row[1] == file_hash:
                print(f"‚úÖ –§–∞–π–ª {file_path} –Ω–µ –±—ã–ª –∏–∑–º–µ–Ω–µ–Ω, –ø—Ä–æ–ø—É—Å–∫")
                return
            
            if row:
                file_id = row[0]
                cursor.execute("DELETE FROM documents WHERE id_file = %s;", (file_id,))
                cursor.execute("DELETE FROM files WHERE id = %s;", (file_id,))
                print(f"‚ôªÔ∏è {file_path} –∏–∑–º–µ–Ω–∏–ª—Å—è, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º —á–∞–Ω–∫–∏")

            cursor.execute(
                """
                INSERT INTO files (source, file_hash, processed_at)
                VALUES (%s, %s, NOW())
                RETURNING id;
                """, 
                (file_path, file_hash)
            )
            file_id = cursor.fetchone()[0]

            contents = [t["page_content"] for t in texts]

            all_embeddings = []
            batch_size = 100
            for i in range(0, len(contents), batch_size):
                batch = contents[i:i + batch_size]
                batch_embeddings = self.model.embed_batch(batch)
                all_embeddings.extend(batch_embeddings)

            try:
                for text, embedding in zip(texts, all_embeddings):
                    cursor.execute(
                        """
                        INSERT INTO documents (id_file, content, embedding, metadata)
                        VALUES (%s, %s, %s, %s);
                        """,
                        (file_id, text["page_content"], Vector(embedding), Json(text["metadata"]))
                    )
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ {e}")

        print(f"üìÑ –§–∞–π–ª {file_path} —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω ({len(texts)} —á–∞–Ω–∫–æ–≤)")

        self.conn.commit()